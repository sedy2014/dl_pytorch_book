{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x28b4ffa82f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "torch.set_printoptions(edgeitems=2)\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['airplane','automobile','bird','cat','deer',\n",
    "               'dog','frog','horse','ship','truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "data_path = '../data-unversioned/p1ch7/'\n",
    "cifar10 = datasets.CIFAR10(\n",
    "    data_path, train=True, download=False,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_val = datasets.CIFAR10(\n",
    "    data_path, train=False, download=False,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0: 0, 2: 1}\n",
    "class_names = ['airplane', 'bird']\n",
    "cifar2 = [(img, label_map[label])\n",
    "          for img, label in cifar10 \n",
    "          if label in [0, 2]]\n",
    "cifar2_val = [(img, label_map[label])\n",
    "              for img, label in cifar10_val\n",
    "              if label in [0, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "n_out = 2\n",
    "\n",
    "model = nn.Sequential(\n",
    "            nn.Linear(\n",
    "                3072,  # <1>\n",
    "                512,   # <2>\n",
    "            ),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(\n",
    "                512,   # <2>\n",
    "                n_out, # <3>\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0900, 0.2447, 0.6652])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "\n",
    "softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(x).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0900, 0.2447, 0.6652],\n",
       "        [0.6652, 0.2447, 0.0900]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "\n",
    "x = torch.tensor([[1.0, 2.0, 3.0],\n",
    "                  [3.0, 2.0, 1.0]])\n",
    "\n",
    "softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "            nn.Linear(3072, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 2),\n",
    "            nn.Softmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, _ = cifar2[0]\n",
    "\n",
    "plt.imshow(img.permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_batch = img.view(-1).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4784, 0.5216]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(img_batch)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, index = torch.max(out, dim=1)\n",
    "\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "            nn.Linear(3072, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 2),\n",
    "            nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5077, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, label = cifar2[0]\n",
    "out = model(img.view(-1).unsqueeze(0))\n",
    "loss(out, torch.tensor([label]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 7.933513\n",
      "Epoch: 1, Loss: 4.865759\n",
      "Epoch: 2, Loss: 3.412104\n",
      "Epoch: 3, Loss: 9.836231\n",
      "Epoch: 4, Loss: 10.781808\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "model = nn.Sequential(\n",
    "            nn.Linear(3072, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 2),\n",
    "            nn.LogSoftmax(dim=1))\n",
    "\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.NLLLoss()\n",
    "n_epochs = 5\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for img, label in cifar2:\n",
    "        img_tensor = img.view(-1).unsqueeze(0)\n",
    "        label_tensor = torch.tensor([label])\n",
    "        out = model(img_tensor)\n",
    "        loss = loss_fn(out, label_tensor)\n",
    "                \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\howardhuang\\AppData\\Local\\Temp\\ipykernel_49148\\9364895.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label_tensor = torch.tensor(labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.318367\n",
      "Epoch: 1, Loss: 0.389473\n",
      "Epoch: 2, Loss: 0.427979\n",
      "Epoch: 3, Loss: 0.435714\n",
      "Epoch: 4, Loss: 0.504344\n",
      "Epoch: 5, Loss: 0.483689\n",
      "Epoch: 6, Loss: 0.308372\n",
      "Epoch: 7, Loss: 0.115488\n",
      "Epoch: 8, Loss: 0.513510\n",
      "Epoch: 9, Loss: 0.386493\n",
      "Epoch: 10, Loss: 0.274975\n",
      "Epoch: 11, Loss: 0.252231\n",
      "Epoch: 12, Loss: 0.213218\n",
      "Epoch: 13, Loss: 0.256708\n",
      "Epoch: 14, Loss: 0.316772\n",
      "Epoch: 15, Loss: 0.631465\n",
      "Epoch: 16, Loss: 0.423487\n",
      "Epoch: 17, Loss: 0.402729\n",
      "Epoch: 18, Loss: 0.307367\n",
      "Epoch: 19, Loss: 0.179422\n",
      "Epoch: 20, Loss: 0.379566\n",
      "Epoch: 21, Loss: 0.290748\n",
      "Epoch: 22, Loss: 0.270363\n",
      "Epoch: 23, Loss: 0.186533\n",
      "Epoch: 24, Loss: 0.285739\n",
      "Epoch: 25, Loss: 0.455766\n",
      "Epoch: 26, Loss: 0.263250\n",
      "Epoch: 27, Loss: 0.257489\n",
      "Epoch: 28, Loss: 0.253201\n",
      "Epoch: 29, Loss: 0.088151\n",
      "Epoch: 30, Loss: 0.119370\n",
      "Epoch: 31, Loss: 0.101625\n",
      "Epoch: 32, Loss: 0.247492\n",
      "Epoch: 33, Loss: 0.090906\n",
      "Epoch: 34, Loss: 0.172537\n",
      "Epoch: 35, Loss: 0.462267\n",
      "Epoch: 36, Loss: 0.273000\n",
      "Epoch: 37, Loss: 0.254223\n",
      "Epoch: 38, Loss: 0.348747\n",
      "Epoch: 39, Loss: 0.300148\n",
      "Epoch: 40, Loss: 0.063681\n",
      "Epoch: 41, Loss: 0.079803\n",
      "Epoch: 42, Loss: 0.135370\n",
      "Epoch: 43, Loss: 0.100524\n",
      "Epoch: 44, Loss: 0.055207\n",
      "Epoch: 45, Loss: 0.177615\n",
      "Epoch: 46, Loss: 0.146831\n",
      "Epoch: 47, Loss: 0.030244\n",
      "Epoch: 48, Loss: 0.113541\n",
      "Epoch: 49, Loss: 0.054586\n",
      "Epoch: 50, Loss: 0.088400\n",
      "Epoch: 51, Loss: 0.037736\n",
      "Epoch: 52, Loss: 0.044950\n",
      "Epoch: 53, Loss: 0.045306\n",
      "Epoch: 54, Loss: 0.122482\n",
      "Epoch: 55, Loss: 0.094259\n",
      "Epoch: 56, Loss: 0.113845\n",
      "Epoch: 57, Loss: 0.020960\n",
      "Epoch: 58, Loss: 0.047257\n",
      "Epoch: 59, Loss: 0.054770\n",
      "Epoch: 60, Loss: 0.057607\n",
      "Epoch: 61, Loss: 0.045996\n",
      "Epoch: 62, Loss: 0.019060\n",
      "Epoch: 63, Loss: 0.017909\n",
      "Epoch: 64, Loss: 0.062859\n",
      "Epoch: 65, Loss: 0.024708\n",
      "Epoch: 66, Loss: 0.028053\n",
      "Epoch: 67, Loss: 0.061581\n",
      "Epoch: 68, Loss: 0.025352\n",
      "Epoch: 69, Loss: 0.021476\n",
      "Epoch: 70, Loss: 0.015957\n",
      "Epoch: 71, Loss: 0.024176\n",
      "Epoch: 72, Loss: 0.139968\n",
      "Epoch: 73, Loss: 0.025018\n",
      "Epoch: 74, Loss: 0.017756\n",
      "Epoch: 75, Loss: 0.012510\n",
      "Epoch: 76, Loss: 0.062693\n",
      "Epoch: 77, Loss: 0.019162\n",
      "Epoch: 78, Loss: 0.026831\n",
      "Epoch: 79, Loss: 0.022641\n",
      "Epoch: 80, Loss: 0.006038\n",
      "Epoch: 81, Loss: 0.012924\n",
      "Epoch: 82, Loss: 0.010721\n",
      "Epoch: 83, Loss: 0.002646\n",
      "Epoch: 84, Loss: 0.017216\n",
      "Epoch: 85, Loss: 0.010292\n",
      "Epoch: 86, Loss: 0.009473\n",
      "Epoch: 87, Loss: 0.034917\n",
      "Epoch: 88, Loss: 0.013506\n",
      "Epoch: 89, Loss: 0.009394\n",
      "Epoch: 90, Loss: 0.010806\n",
      "Epoch: 91, Loss: 0.015954\n",
      "Epoch: 92, Loss: 0.011492\n",
      "Epoch: 93, Loss: 0.019395\n",
      "Epoch: 94, Loss: 0.011841\n",
      "Epoch: 95, Loss: 0.025058\n",
      "Epoch: 96, Loss: 0.015301\n",
      "Epoch: 97, Loss: 0.008132\n",
      "Epoch: 98, Loss: 0.011373\n",
      "Epoch: 99, Loss: 0.013405\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
    "                                           shuffle=True)\n",
    "\n",
    "model = nn.Sequential(\n",
    "            nn.Linear(3072, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 2),\n",
    "            nn.LogSoftmax(dim=1))\n",
    "\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.NLLLoss()\n",
    "n_epochs = 100\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for imgs, labels in train_loader:\n",
    "        batch_size = imgs.shape[0]\n",
    "        img_tensor = imgs.view(batch_size, -1)\n",
    "        label_tensor = torch.tensor(labels)\n",
    "        out = model(img_tensor)\n",
    "        loss = loss_fn(out, label_tensor)\n",
    "                \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.810500\n"
     ]
    }
   ],
   "source": [
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64,\n",
    "                                         shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "        \n",
    "print(\"Accuracy: %f\" % (correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "            nn.Linear(3072, 1024),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 2),\n",
    "            nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "            nn.Linear(3072, 1024),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 2))\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\howardhuang\\AppData\\Local\\Temp\\ipykernel_49148\\3226406213.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label_tensor = torch.tensor(labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.571264\n",
      "Epoch: 1, Loss: 0.313634\n",
      "Epoch: 2, Loss: 0.317605\n",
      "Epoch: 3, Loss: 0.339209\n",
      "Epoch: 4, Loss: 0.396129\n",
      "Epoch: 5, Loss: 0.502080\n",
      "Epoch: 6, Loss: 0.272827\n",
      "Epoch: 7, Loss: 0.275626\n",
      "Epoch: 8, Loss: 0.294260\n",
      "Epoch: 9, Loss: 0.277196\n",
      "Epoch: 10, Loss: 0.588599\n",
      "Epoch: 11, Loss: 0.679920\n",
      "Epoch: 12, Loss: 0.694684\n",
      "Epoch: 13, Loss: 0.216509\n",
      "Epoch: 14, Loss: 0.257251\n",
      "Epoch: 15, Loss: 0.367354\n",
      "Epoch: 16, Loss: 0.215198\n",
      "Epoch: 17, Loss: 0.159641\n",
      "Epoch: 18, Loss: 0.293755\n",
      "Epoch: 19, Loss: 0.328064\n",
      "Epoch: 20, Loss: 0.375317\n",
      "Epoch: 21, Loss: 0.381251\n",
      "Epoch: 22, Loss: 0.421697\n",
      "Epoch: 23, Loss: 0.186160\n",
      "Epoch: 24, Loss: 0.388323\n",
      "Epoch: 25, Loss: 0.295580\n",
      "Epoch: 26, Loss: 0.168410\n",
      "Epoch: 27, Loss: 0.273761\n",
      "Epoch: 28, Loss: 0.075763\n",
      "Epoch: 29, Loss: 0.226366\n",
      "Epoch: 30, Loss: 0.097388\n",
      "Epoch: 31, Loss: 0.134953\n",
      "Epoch: 32, Loss: 0.083903\n",
      "Epoch: 33, Loss: 0.213210\n",
      "Epoch: 34, Loss: 0.077667\n",
      "Epoch: 35, Loss: 0.294085\n",
      "Epoch: 36, Loss: 0.466938\n",
      "Epoch: 37, Loss: 0.069569\n",
      "Epoch: 38, Loss: 0.337082\n",
      "Epoch: 39, Loss: 0.043604\n",
      "Epoch: 40, Loss: 0.394409\n",
      "Epoch: 41, Loss: 0.089557\n",
      "Epoch: 42, Loss: 0.143271\n",
      "Epoch: 43, Loss: 0.041468\n",
      "Epoch: 44, Loss: 0.034691\n",
      "Epoch: 45, Loss: 0.156162\n",
      "Epoch: 46, Loss: 0.068570\n",
      "Epoch: 47, Loss: 0.209330\n",
      "Epoch: 48, Loss: 0.018960\n",
      "Epoch: 49, Loss: 0.043673\n",
      "Epoch: 50, Loss: 0.128867\n",
      "Epoch: 51, Loss: 0.099857\n",
      "Epoch: 52, Loss: 0.832665\n",
      "Epoch: 53, Loss: 0.016201\n",
      "Epoch: 54, Loss: 0.060363\n",
      "Epoch: 55, Loss: 0.012793\n",
      "Epoch: 56, Loss: 0.006818\n",
      "Epoch: 57, Loss: 0.015441\n",
      "Epoch: 58, Loss: 0.035839\n",
      "Epoch: 59, Loss: 0.007867\n",
      "Epoch: 60, Loss: 0.016963\n",
      "Epoch: 61, Loss: 0.006292\n",
      "Epoch: 62, Loss: 0.011022\n",
      "Epoch: 63, Loss: 0.007652\n",
      "Epoch: 64, Loss: 0.052086\n",
      "Epoch: 65, Loss: 0.047699\n",
      "Epoch: 66, Loss: 0.232182\n",
      "Epoch: 67, Loss: 0.017223\n",
      "Epoch: 68, Loss: 0.003991\n",
      "Epoch: 69, Loss: 0.002515\n",
      "Epoch: 70, Loss: 0.011756\n",
      "Epoch: 71, Loss: 0.003628\n",
      "Epoch: 72, Loss: 0.009310\n",
      "Epoch: 73, Loss: 0.002574\n",
      "Epoch: 74, Loss: 0.001901\n",
      "Epoch: 75, Loss: 0.002902\n",
      "Epoch: 76, Loss: 0.006603\n",
      "Epoch: 77, Loss: 0.002843\n",
      "Epoch: 78, Loss: 0.054683\n",
      "Epoch: 79, Loss: 0.005647\n",
      "Epoch: 80, Loss: 0.003988\n",
      "Epoch: 81, Loss: 0.003243\n",
      "Epoch: 82, Loss: 0.005832\n",
      "Epoch: 83, Loss: 0.000351\n",
      "Epoch: 84, Loss: 0.002901\n",
      "Epoch: 85, Loss: 0.000627\n",
      "Epoch: 86, Loss: 0.818654\n",
      "Epoch: 87, Loss: 0.007514\n",
      "Epoch: 88, Loss: 0.002616\n",
      "Epoch: 89, Loss: 0.003321\n",
      "Epoch: 90, Loss: 0.001032\n",
      "Epoch: 91, Loss: 0.001379\n",
      "Epoch: 92, Loss: 0.002112\n",
      "Epoch: 93, Loss: 0.003041\n",
      "Epoch: 94, Loss: 0.001571\n",
      "Epoch: 95, Loss: 0.000677\n",
      "Epoch: 96, Loss: 0.002345\n",
      "Epoch: 97, Loss: 0.003105\n",
      "Epoch: 98, Loss: 0.004900\n",
      "Epoch: 99, Loss: 0.004908\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
    "                                           shuffle=True)\n",
    "\n",
    "model = nn.Sequential(\n",
    "            nn.Linear(3072, 1024),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 2))\n",
    "\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "n_epochs = 100\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for imgs, labels in train_loader:\n",
    "        batch_size = imgs.shape[0]\n",
    "        img_tensor = imgs.view(batch_size, -1)\n",
    "        label_tensor = torch.tensor(labels)\n",
    "        out = model(img_tensor)\n",
    "        loss = loss_fn(out, label_tensor)\n",
    "                \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.000000\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
    "                                           shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in train_loader:\n",
    "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "        \n",
    "print(\"Accuracy: %f\" % (correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.805500\n"
     ]
    }
   ],
   "source": [
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64,\n",
    "                                         shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "        \n",
    "print(\"Accuracy: %f\" % (correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3737474, [3145728, 1024, 524288, 512, 65536, 128, 256, 2])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numel_list = [p.numel()\n",
    "              for p in model.parameters()\n",
    "              if p.requires_grad == True]\n",
    "sum(numel_list), numel_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1574402, [1572864, 512, 1024, 2])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_model = nn.Sequential(\n",
    "                nn.Linear(3072, 512),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(512, 2),\n",
    "                nn.LogSoftmax(dim=1))\n",
    "\n",
    "numel_list = [p.numel() for p in first_model.parameters()]\n",
    "sum(numel_list), numel_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1024, 3072]), torch.Size([1024]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = nn.Linear(3072, 1024)\n",
    "\n",
    "linear.weight.shape, linear.bias.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
