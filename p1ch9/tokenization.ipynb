{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "\n",
    "# Create a word-level tokenizer\n",
    "tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "# Use whitespace pre-tokenizer\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "# Define the trainer\n",
    "trainer = WordLevelTrainer(special_tokens=[\"[UNK]\"])\n",
    "tokenizer.train([\"../data/p2ch9/odyssey.txt\"], trainer=trainer)\n",
    "\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "decode = tokenizer.decode\n",
    "encode = tokenizer.encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the text file and read the lines\n",
    "with open('../data/p2ch9/odyssey.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "# Tokenize the text file\n",
    "encoding = tokenizer.encode(text)\n",
    "# Create sequences of 100 tokens\n",
    "sequence_length = 100\n",
    "X, Y = [], []\n",
    "for i in range(0, len(encoding.ids) - sequence_length, sequence_length):\n",
    "    X.append(encoding.ids[i:i+sequence_length])\n",
    "    Y.append(encoding.ids[i+1:i+sequence_length+1])\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "X = torch.tensor(X).to(device)\n",
    "Y = torch.tensor(Y).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================== PREVIOUS CODE ===============================\n",
    "def get_batch(batch_size=64):\n",
    "    random_idx = torch.randint(0, X.size(0), (batch_size,))\n",
    "    batch = X[random_idx]\n",
    "    labels = Y[random_idx]\n",
    "    return batch, labels\n",
    "batch, labels = get_batch()\n",
    "\n",
    "def train(model, optimizer, num_steps=10_001, loss_report_interval=1_000):\n",
    "    losses = []\n",
    "    for i in range(1, num_steps):\n",
    "        inputs, labels = get_batch()\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(inputs)\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), labels.view(-1), ignore_index=-1)\n",
    "        losses.append(loss.item())\n",
    "        if i % loss_report_interval == 0:\n",
    "            print(f'Average loss at step {i}: {sum(losses[-loss_report_interval:]) / loss_report_interval:.4f}')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def generate_samples(model, num_samples=1, max_len=sequence_length):\n",
    "    sequences = torch.zeros((num_samples, 1)).int()\n",
    "    for _ in range(max_len):\n",
    "        logits = model(sequences)\n",
    "        logits = logits[:, -1, :]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        sequences = torch.cat((sequences, idx_next), dim=1)\n",
    "\n",
    "    for sequence in sequences:\n",
    "        indices = torch.where(sequence == 0)[0]\n",
    "        end = indices[1] if len(indices) > 1 else max_len\n",
    "        sequence = sequence[1:end]\n",
    "        print(decode(sequence))\n",
    "# =============================== FINISH PREVIOUS CODE ==============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, n_embd, num_heads=4, n_hidden=64):\n",
    "        super().__init__()\n",
    "        assert n_embd % num_heads == 0, \"Embedding dimension must be divisible by the number of heads\"\n",
    "\n",
    "        self.num_heads = num_heads # <1>\n",
    "        self.head_dim = n_embd // num_heads\n",
    "\n",
    "        self.query_proj = nn.Linear(n_embd, n_embd)\n",
    "        self.key_proj = nn.Linear(n_embd, n_embd)\n",
    "        self.value_proj = nn.Linear(n_embd, n_embd)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_embd)\n",
    "        )\n",
    "\n",
    "        self.norm_1 = nn.LayerNorm(n_embd)\n",
    "        self.norm_2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, sequence_length, _ = x.shape\n",
    "\n",
    "        q = self.query_proj(x)\n",
    "        k = self.key_proj(x)\n",
    "        v = self.value_proj(x)\n",
    "\n",
    "        # multiheaded attention\n",
    "        q = q.view(batch_size, sequence_length, self.num_heads, self.head_dim).transpose(1, 2) # <1>\n",
    "        k = k.view(batch_size, sequence_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(batch_size, sequence_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # attention\n",
    "        attn_weights = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "\n",
    "        # multiple heads concatenation\n",
    "        attn_weights = attn_weights.transpose(1, 2).contiguous().view(batch_size, sequence_length, -1)\n",
    "\n",
    "        # norm and residual connections here\n",
    "        x = self.norm_1(x + attn_weights)\n",
    "        x = self.norm_2(x + self.mlp(x))\n",
    "        return x\n",
    "    \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, n_embd, vocab_size, block_size, num_blocks=6):\n",
    "        super().__init__()\n",
    "        self.char_embedding = nn.Embedding(vocab_size, n_embd)\n",
    "        self.positional_embedding = nn.Embedding(block_size, n_embd)\n",
    "\n",
    "        self.transformer_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(n_embd) for _ in range(num_blocks)]\n",
    "        )\n",
    "\n",
    "        self.output_proj = nn.Linear(n_embd, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        _, seq_len = x.shape\n",
    "\n",
    "        pos_embd = self.positional_embedding(torch.arange(seq_len).to(device)) #  <1>\n",
    "        char_embd = self.char_embedding(x)\n",
    "        x = char_embd + pos_embd\n",
    "        x = self.transformer_blocks(x)\n",
    "        x = self.output_proj(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 64\n",
    "model = Transformer(n_embd, vocab_size, block_size=sequence_length)\n",
    "model.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "train(model, optimizer, num_steps=501, loss_report_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 200: 5.6415\n",
      "Average loss at step 400: 5.6001\n",
      "Average loss at step 600: 5.5660\n",
      "Average loss at step 800: 5.5303\n",
      "Average loss at step 1000: 5.4998\n",
      "Average loss at step 1200: 5.4680\n",
      "Average loss at step 1400: 5.4355\n",
      "Average loss at step 1600: 5.4117\n",
      "Average loss at step 1800: 5.3868\n",
      "Average loss at step 2000: 5.3663\n",
      "Average loss at step 2200: 5.3272\n",
      "Average loss at step 2400: 5.3095\n",
      "Average loss at step 2600: 5.2882\n",
      "Average loss at step 2800: 5.2673\n",
      "Average loss at step 3000: 5.2418\n",
      "Average loss at step 3200: 5.2128\n",
      "Average loss at step 3400: 5.1968\n",
      "Average loss at step 3600: 5.1682\n",
      "Average loss at step 3800: 5.1526\n",
      "Average loss at step 4000: 5.1307\n",
      "Average loss at step 4200: 5.1172\n",
      "Average loss at step 4400: 5.0911\n",
      "Average loss at step 4600: 5.0676\n",
      "Average loss at step 4800: 5.0463\n",
      "Average loss at step 5000: 5.0262\n"
     ]
    }
   ],
   "source": [
    "# If you have cuda available, train for longer\n",
    "train(model, optimizer, num_steps=5001, loss_report_interval=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day horses and as she were dream to do.  twittering spake Ulysses laid fain.  He fear.  Man for their choice,  for the councils and one Hebe angrily πύματον treating which?  61 140 they went - offering in the son of all before the island,  and saw mother ’,  would have a set it did a sweet that we could already,  see the Cnossus mortal - rejoinder ahead into your country to quantity of it,  till Ulysses to Troy,  nor just in us who was as he heard his eyelids\n"
     ]
    }
   ],
   "source": [
    "def generate_samples(model, num_samples=1, max_len=sequence_length):\n",
    "    sequences = torch.zeros((num_samples, 1)).int().to(device)\n",
    "    for _ in range(max_len):\n",
    "        logits = model(sequences)\n",
    "        logits = logits[:, -1, :]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        sequences = torch.cat((sequences, idx_next), dim=1)\n",
    "\n",
    "    for sequence in sequences:\n",
    "        indices = torch.where(sequence == 0)[0]\n",
    "        end = indices[1] if len(indices) > 1 else max_len\n",
    "        sequence = sequence[1:end]\n",
    "        decoded_sequence = decode(sequence.tolist())\n",
    "        print(format_sequence(decoded_sequence))\n",
    "\n",
    "def format_sequence(sequence):\n",
    "    # This function formats the sequence to handle punctuation and spacing correctly.\n",
    "    formatted_sequence = \"\"\n",
    "    for i, char in enumerate(sequence):\n",
    "        if char in \",.;:!?\":\n",
    "            formatted_sequence = formatted_sequence.rstrip() + char + \" \"\n",
    "        else:\n",
    "            formatted_sequence += char\n",
    "    return formatted_sequence.strip()\n",
    "\n",
    "generate_samples(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
